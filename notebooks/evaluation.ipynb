{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook - Image â†’ Music Generation\n",
    "\n",
    "Quick evaluation of the music generation system.\n",
    "\n",
    "## What's in here\n",
    "1. Load model\n",
    "2. Generate samples\n",
    "3. Check diversity\n",
    "4. Check musicality\n",
    "5. Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from midi_tokenizer import MIDITokenizer\n",
    "from model import ConditionalTransformer\n",
    "from evaluate import calculate_diversity_metrics, calculate_musicality_metrics\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MIDITokenizer.load_vocab('../data/processed/vocab.json')\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "checkpoint_path = '../models/best_model.pt'\n",
    "if Path(checkpoint_path).exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Train a model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(checkpoint_path).exists():\n",
    "    config = checkpoint['config']['model']\n",
    "    model = ConditionalTransformer(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        **config\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    emotions = ['happy', 'sad', 'calm', 'angry', 'surprised']\n",
    "    samples_per_emotion = 5\n",
    "    \n",
    "    all_samples = []\n",
    "    \n",
    "    for emotion_id, emotion in enumerate(emotions):\n",
    "        print(f\"Generating {emotion} melodies...\")\n",
    "        for i in range(samples_per_emotion):\n",
    "            with torch.no_grad():\n",
    "                image_embed = torch.randn(1, 512)\n",
    "                emotion_label = torch.tensor([emotion_id])\n",
    "                \n",
    "                tokens = model.generate(\n",
    "                    image_embed,\n",
    "                    emotion_label,\n",
    "                    max_length=128,\n",
    "                    temperature=0.9,\n",
    "                    top_k=40,\n",
    "                    bos_token=tokenizer.bos_id,\n",
    "                    eos_token=tokenizer.eos_id\n",
    "                )\n",
    "                \n",
    "                all_samples.append({\n",
    "                    'emotion': emotion,\n",
    "                    'tokens': tokens[0].tolist()\n",
    "                })\n",
    "    \n",
    "    print(f\"Generated {len(all_samples)} total samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(checkpoint_path).exists():\n",
    "    token_sequences = [s['tokens'] for s in all_samples]\n",
    "    diversity = calculate_diversity_metrics(token_sequences)\n",
    "    \n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in diversity.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    metrics = list(diversity.keys())\n",
    "    values = list(diversity.values())\n",
    "    \n",
    "    ax.bar(metrics, values, color='steelblue')\n",
    "    ax.set_ylabel('Ratio')\n",
    "    ax.set_title('Diversity Metrics')\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Musicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(checkpoint_path).exists():\n",
    "    musicality = calculate_musicality_metrics(token_sequences, tokenizer)\n",
    "    \n",
    "    print(\"\\nMusicality Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in musicality.items():\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].bar(['Avg Notes'], [musicality['avg_notes_per_sequence']], color='coral')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Average Notes per Sequence')\n",
    "    \n",
    "    axes[1].bar(['Avg Interval'], [musicality['avg_melodic_interval']], color='lightgreen')\n",
    "    axes[1].set_ylabel('Semitones')\n",
    "    axes[1].set_title('Average Melodic Interval')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Emotion comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(checkpoint_path).exists():\n",
    "    emotion_stats = {}\n",
    "    \n",
    "    for emotion in emotions:\n",
    "        emotion_samples = [s['tokens'] for s in all_samples if s['emotion'] == emotion]\n",
    "        \n",
    "        if emotion_samples:\n",
    "            metrics = calculate_musicality_metrics(emotion_samples, tokenizer)\n",
    "            emotion_stats[emotion] = metrics\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    emotions_list = list(emotion_stats.keys())\n",
    "    note_counts = [emotion_stats[e]['avg_notes_per_sequence'] for e in emotions_list]\n",
    "    \n",
    "    colors = ['gold', 'steelblue', 'lightgreen', 'tomato', 'purple']\n",
    "    ax.bar(emotions_list, note_counts, color=colors)\n",
    "    ax.set_ylabel('Average Notes')\n",
    "    ax.set_title('Note Density by Emotion')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(checkpoint_path).exists() and 'history' in checkpoint:\n",
    "    history = checkpoint['history']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], label='Train Loss', marker='o')\n",
    "    ax1.plot(epochs, history['val_loss'], label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, history['learning_rates'], marker='o', color='green')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Schedule')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Quick evaluation metrics:\n",
    "- Load trained models\n",
    "- Generate music for different emotions\n",
    "- Check diversity (n-gram uniqueness)\n",
    "- Check musicality (note density, intervals)\n",
    "- Visualize results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
