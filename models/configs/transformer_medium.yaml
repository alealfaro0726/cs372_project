# Medium Transformer configuration for better quality music generation
# Suitable for large datasets (100K+ sequences)

model:
  name: "transformer_medium"
  d_model: 512              # Full embedding dimension
  n_heads: 8                # Standard number of attention heads
  n_layers: 4               # More layers for complex patterns
  d_ff: 2048                # Full feed-forward dimension
  dropout: 0.15             # Slightly higher dropout for regularization
  max_seq_len: 512          # Maximum sequence length

  # Conditioning
  use_image_conditioning: true
  use_emotion_conditioning: true
  image_embed_dim: 512      # CLIP embedding size
  emotion_embed_dim: 64     # Learned emotion embedding size

training:
  epochs: 25                # Enough for convergence on large dataset
  batch_size: 32            # Larger batch for stability
  learning_rate: 0.0002     # Slightly lower for larger model
  weight_decay: 0.01        # L2 regularization

  # Learning rate schedule
  warmup_steps: 1000
  lr_scheduler: "cosine"    # Options: cosine, plateau, step

  # Optimization
  optimizer: "adamw"        # AdamW optimizer
  grad_clip: 1.0            # Gradient clipping threshold
  mixed_precision: true     # Use AMP if GPU available

  # Regularization
  label_smoothing: 0.1      # Label smoothing

  # Checkpointing
  save_every: 5             # Save checkpoint every N epochs
  eval_every: 1             # Evaluate every N epochs
  early_stopping_patience: 10

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

  # Data augmentation
  augment_transpose: true   # Random pitch transposition
  transpose_range: [-3, 3]  # Semitones

  augment_tempo: true       # Random tempo scaling
  tempo_range: [0.9, 1.1]   # Ratio

sampling:
  temperature: 1.0          # Balanced creativity
  top_k: 40                 # Balanced variety
  top_p: 0.9                # Nucleus sampling
  max_length: 256           # Maximum generation length

seed: 42                    # Random seed for reproducibility

logging:
  log_dir: "logs"
  tensorboard: false        # Enable TensorBoard logging
  wandb: false              # Enable Weights & Biases
  log_interval: 100         # Log every N batches
